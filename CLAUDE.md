# Claude Code Instructions - Narrative Assistant (TFM)

## üîë Permisos y Autorizaci√≥n

**Claude tiene PERMISO COMPLETO para**:
- ‚úÖ Buscar, leer y analizar cualquier archivo del proyecto
- ‚úÖ Ejecutar c√≥digo, scripts, tests
- ‚úÖ Instalar dependencias y paquetes
- ‚úÖ Modificar, crear y eliminar archivos
- ‚úÖ Ejecutar comandos de sistema (git, npm, pip, etc.)
- ‚úÖ Revisar bases de datos y logs
- ‚úÖ Hacer cambios arquitect√≥nicos cuando sea necesario
- ‚úÖ Refactorizar c√≥digo sin preguntar previamente
- ‚úÖ Eliminar c√≥digo muerto o duplicado
- ‚úÖ Crear y modificar tests

**NO es necesario pedir permiso antes de**:
- Revisar o explorar el c√≥digo
- Ejecutar tests o an√°lisis
- Hacer b√∫squedas exhaustivas
- Instalar herramientas necesarias
- Corregir bugs evidentes
- Mejorar logging o debugging

**Solo preguntar cuando**:
- Hay m√∫ltiples enfoques arquitect√≥nicos v√°lidos con trade-offs importantes
- Se va a eliminar funcionalidad existente (no c√≥digo muerto)
- Se necesita aclarar requisitos del usuario

---

## Setup R√°pido (Nueva M√°quina)

**IMPORTANTE**: Solo copiar la carpeta del proyecto. Los modelos est√°n incluidos localmente.

```bash
# 1. Crear entorno virtual
cd tfm
python3.11 -m venv .venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows

# 2. Instalar dependencias
pip install -e ".[dev]"

# 3. Instalar/Configurar Ollama para LLM local
python scripts/setup_ollama.py

# 4. Verificar (NO requiere internet)
python scripts/verify_environment.py
# o
narrative-assistant verify
```

**No se necesita `spacy download`** - el modelo est√° en `models/spacy/`.
**Ollama se instala y configura con** `setup_ollama.py` - descarga modelos locales autom√°ticamente.

---

## Proyecto

**Asistente de Correcci√≥n Narrativa** - Herramienta de asistencia a correctores literarios profesionales para detectar inconsistencias en manuscritos de ficci√≥n.

### Stack Tecnol√≥gico
- **Python** 3.11+
- **NLP**: spaCy (es_core_news_lg), sentence-transformers
- **LLM Local**: Ollama (llama3.2, mistral, qwen2.5)
- **GPU**: PyTorch (CUDA/MPS/CPU auto-detect)
- **DB**: SQLite con WAL mode
- **Formatos**: DOCX (prioritario), TXT, MD, PDF, EPUB

### Requisito de Red
- **√önico acceso a internet**: Sistema de licencias (verificaci√≥n) e instalaci√≥n inicial de Ollama
- **Modelos NLP y LLM**: 100% offline desde `models/` y Ollama local

---

## Modelos Locales

Los modelos se almacenan en el proyecto para funcionamiento offline:

```
tfm/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ spacy/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ es_core_news_lg/     # Modelo spaCy espa√±ol (~500 MB)
‚îÇ   ‚îî‚îÄ‚îÄ embeddings/
‚îÇ       ‚îî‚îÄ‚îÄ paraphrase-multilingual-MiniLM-L12-v2/  # sentence-transformers (~500 MB)
```

### Primera vez: Descargar modelos
Si `models/` no existe (primera instalaci√≥n o proyecto nuevo):
```bash
python scripts/download_models.py
```
Esto descarga los modelos (~1 GB total) y los guarda en `models/`.

### Configuraci√≥n autom√°tica
El sistema busca modelos en este orden:
1. `./models/` (proyecto local) - **PREFERIDO**
2. `~/.narrative_assistant/models/` (usuario)
3. Cache de HuggingFace/spaCy (requiere internet)

### Variables de entorno para modelos locales
```bash
# Opcional - por defecto usa ./models/
NA_SPACY_MODEL_PATH=./models/spacy/es_core_news_lg
NA_EMBEDDINGS_MODEL_PATH=./models/embeddings/paraphrase-multilingual-MiniLM-L12-v2
```

---

## Ollama / LLM Local

Ollama proporciona an√°lisis sem√°ntico avanzado mediante modelos LLM que corren 100% localmente.

### Instalaci√≥n
```bash
# Autom√°tica (recomendado)
python scripts/setup_ollama.py

# Manual - Windows
# Descargar desde https://ollama.com/download

# Manual - Linux
curl -fsSL https://ollama.com/install.sh | sh

# Manual - macOS
brew install ollama
```

### Modelos disponibles

| Modelo | Tama√±o | Velocidad | Calidad | Notas |
|--------|--------|-----------|---------|-------|
| `llama3.2` | 3B | R√°pido | Buena | **Default**, funciona en CPU |
| `mistral` | 7B | Media | Alta | Mejor razonamiento |
| `qwen2.5` | 7B | Media | Alta | Excelente para espa√±ol |
| `gemma2` | 9B | Lento | Muy alta | Requiere m√°s recursos |

### Descargar modelos
```bash
ollama pull llama3.2     # Recomendado (~2 GB)
ollama pull qwen2.5      # Opcional, mejor espa√±ol (~4 GB)
ollama pull mistral      # Opcional, mayor calidad (~4 GB)
```

### Iniciar servicio
```bash
ollama serve  # Corre en localhost:11434
```

### Sistema Multi-Modelo (Votaci√≥n)
El an√°lisis de comportamiento de personajes puede usar m√∫ltiples m√©todos:

1. **Modelos LLM** (llama3.2, mistral, qwen2.5, gemma2)
2. **Reglas y heur√≠sticas** (rule_based) - Siempre disponible
3. **Embeddings sem√°nticos** (embeddings) - Similitud vectorial

Configuraci√≥n en Settings:
- **M√©todos habilitados**: Selecci√≥n m√∫ltiple de m√©todos
- **Confianza m√≠nima**: Umbral para mostrar expectativas
- **Consenso m√≠nimo**: Porcentaje de m√©todos que deben coincidir

### Variables de entorno LLM
```bash
NA_LLM_BACKEND=ollama              # Backend: ollama, transformers, none
NA_OLLAMA_HOST=http://localhost:11434  # URL del servidor Ollama
NA_OLLAMA_MODEL=llama3.2           # Modelo por defecto
```

---

## Convenciones de C√≥digo

### Python
- **Estilo**: PEP 8, Black formatter, isort
- **Type hints**: Obligatorios en funciones p√∫blicas
- **Docstrings**: Google style en espa√±ol
- **Imports**: Relativos dentro del paquete (`from ..core.config import ...`)

### Patrones Arquitect√≥nicos

#### Result Pattern
Para operaciones que pueden tener √©xitos parciales:
```python
from narrative_assistant.core import Result

def process(data) -> Result[OutputType]:
    if error_condition:
        return Result.failure(SomeError(...))
    return Result.success(output)
```

#### Singleton Thread-Safe
Todos los singletons usan double-checked locking:
```python
import threading

_lock = threading.Lock()
_instance = None

def get_instance():
    global _instance
    if _instance is None:
        with _lock:
            if _instance is None:
                _instance = create_instance()
    return _instance
```

#### Validaci√≥n de Archivos
Los parsers SIEMPRE validan antes de abrir:
```python
def parse(self, path: Path) -> Result[RawDocument]:
    validation = self.validate_file(path)  # Path traversal, size, extension
    if validation.is_failure:
        return validation
    # ... continuar
```

---

## Estructura de M√≥dulos

```
src/narrative_assistant/
‚îú‚îÄ‚îÄ core/           # Infraestructura base
‚îÇ   ‚îú‚îÄ‚îÄ config.py   # Configuraci√≥n centralizada (GPUConfig, NLPConfig, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ device.py   # Detecci√≥n GPU (CUDA, MPS, CPU)
‚îÇ   ‚îú‚îÄ‚îÄ errors.py   # Jerarqu√≠a de errores (NarrativeError, severity levels)
‚îÇ   ‚îî‚îÄ‚îÄ result.py   # Result[T] pattern
‚îÇ
‚îú‚îÄ‚îÄ persistence/    # Estado y base de datos
‚îÇ   ‚îú‚îÄ‚îÄ database.py           # SQLite, transacciones
‚îÇ   ‚îú‚îÄ‚îÄ project.py            # Proyecto = un manuscrito
‚îÇ   ‚îú‚îÄ‚îÄ document_fingerprint.py  # SHA-256 + n-gram Jaccard
‚îÇ   ‚îú‚îÄ‚îÄ session.py            # Sesi√≥n de trabajo del revisor
‚îÇ   ‚îî‚îÄ‚îÄ history.py            # Historial de cambios (undo/redo)
‚îÇ
‚îú‚îÄ‚îÄ parsers/        # Lectura de documentos
‚îÇ   ‚îú‚îÄ‚îÄ base.py              # DocumentParser ABC, RawDocument
‚îÇ   ‚îú‚îÄ‚îÄ docx_parser.py       # Word (.docx)
‚îÇ   ‚îú‚îÄ‚îÄ txt_parser.py        # Texto plano + Markdown
‚îÇ   ‚îî‚îÄ‚îÄ sanitization.py      # InputSanitizer, validate_file_path
‚îÇ
‚îî‚îÄ‚îÄ nlp/            # Procesamiento NLP
    ‚îú‚îÄ‚îÄ spacy_gpu.py    # setup_spacy_gpu(), load_spacy_model()
    ‚îú‚îÄ‚îÄ embeddings.py   # EmbeddingsModel con fallback OOM
    ‚îú‚îÄ‚îÄ chunking.py     # TextChunker para docs grandes
    ‚îú‚îÄ‚îÄ ner.py          # NERExtractor - extracci√≥n de entidades
    ‚îî‚îÄ‚îÄ coreference_resolver.py  # Sistema de correferencias multi-m√©todo
```

### Sistema de Correferencias (Votaci√≥n Multi-M√©todo)

El sistema usa **4 m√©todos independientes** con votaci√≥n ponderada:

| M√©todo | Peso | Descripci√≥n |
|--------|------|-------------|
| `embeddings` | 30% | Similitud sem√°ntica (sentence-transformers) |
| `llm` | 35% | LLM local (Ollama: llama3.2, mistral, qwen2.5) |
| `morpho` | 20% | An√°lisis morfosint√°ctico (spaCy) |
| `heuristics` | 15% | Heur√≠sticas narrativas (proximidad, patrones) |

**Uso**:
```python
from narrative_assistant.nlp.coreference_resolver import resolve_coreferences_voting

result = resolve_coreferences_voting(text, chapters=chapters_data)
# result.chains -> cadenas de correferencia
# result.unresolved -> menciones sin resolver
```

**Documentaci√≥n completa**: [docs/COREFERENCE_RESOLUTION.md](docs/COREFERENCE_RESOLUTION.md)

---

## Comandos de Desarrollo

```bash
# Verificar entorno (offline)
narrative-assistant verify

# Info del sistema
narrative-assistant info

# Analizar documento
narrative-assistant analyze documento.docx

# Tests
pytest -v

# Formateo
black src/ && isort src/

# Type checking
mypy src/
```

---

## Variables de Entorno

| Variable | Valores | Default | Descripci√≥n |
|----------|---------|---------|-------------|
| `NA_DEVICE` | auto, cuda, mps, cpu | auto | Dispositivo preferido |
| `NA_SPACY_GPU` | true, false | true | Habilitar GPU para spaCy |
| `NA_EMBEDDINGS_GPU` | true, false | true | Habilitar GPU para embeddings |
| `NA_BATCH_SIZE_GPU` | int | 64 | Batch size en GPU |
| `NA_BATCH_SIZE_CPU` | int | 16 | Batch size en CPU |
| `NA_LOG_LEVEL` | DEBUG, INFO, WARNING, ERROR | INFO | Nivel de logging |
| `NA_DATA_DIR` | path | ~/.narrative_assistant | Directorio de datos |
| `NA_SPACY_MODEL_PATH` | path | ./models/spacy/es_core_news_lg | Modelo spaCy local |
| `NA_EMBEDDINGS_MODEL_PATH` | path | ./models/embeddings/... | Modelo embeddings local |

---

## Errores Comunes y Soluciones

### GPU OOM
El sistema tiene fallback autom√°tico a CPU con batch reducido. Si persiste:
```bash
export NA_BATCH_SIZE_GPU=32  # Reducir
# o
export NA_DEVICE=cpu  # Forzar CPU
```

### Modelo no encontrado
Verificar que `models/` est√© copiado con el proyecto:
```bash
ls -la models/spacy/es_core_news_lg/
ls -la models/embeddings/
```

### Import errors
Verificar que el paquete est√° instalado en modo editable:
```bash
pip install -e .
```

---

## Seguridad - Aislamiento de Manuscritos

**CR√çTICO**: Los manuscritos NUNCA deben salir de la m√°quina del usuario.

### Reglas de seguridad obligatorias

1. **Sin acceso a internet** excepto verificaci√≥n de licencias
2. **Modelos NLP solo locales** - fallar si no est√°n en `models/`
3. **Sin telemetr√≠a** ni analytics de ning√∫n tipo
4. **Sin auto-updates** de modelos o dependencias

### Variables de entorno forzadas
```python
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"
```

### Al generar c√≥digo - PROHIBIDO:
- ‚ùå `requests`, `urllib`, `httpx`, `aiohttp`
- ‚ùå Cualquier llamada HTTP/HTTPS (excepto licencias)
- ‚ùå Enviar datos a servicios externos
- ‚ùå Descargas autom√°ticas de modelos
- ‚ùå Analytics, telemetr√≠a, logging remoto

Ver: [docs/02-architecture/SECURITY.md](docs/02-architecture/SECURITY.md)

---

## Notas para Claude

1. **Idioma**: El c√≥digo est√° en ingl√©s, docstrings y comentarios en espa√±ol
2. **Tests**: A√∫n no implementados, priorizar implementaci√≥n
3. **UI**: Vue 3 + PrimeVue (frontend), FastAPI (api-server)
4. **LLM Integration**: Ollama para an√°lisis sem√°ntico local (100% offline)
5. **Offline**: Todos los modelos (NLP y LLM) son locales, no requieren internet

### Al generar c√≥digo:
- Usar type hints siempre
- Seguir Result pattern para operaciones fallibles
- Validar inputs (especialmente paths de archivos)
- A√±adir logging apropiado
- Considerar thread-safety en singletons
- **NO a√±adir c√≥digo que requiera internet** (excepto licencias)
- **Verificar que no hay filtraciones de datos**
