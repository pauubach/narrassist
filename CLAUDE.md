# Claude Code Instructions - Narrative Assistant (TFM)

## ğŸ”‘ Permisos y AutorizaciÃ³n

**Claude tiene PERMISO COMPLETO para**:
- âœ… Buscar, leer y analizar cualquier archivo del proyecto
- âœ… Ejecutar cÃ³digo, scripts, tests
- âœ… Instalar dependencias y paquetes
- âœ… Modificar, crear y eliminar archivos
- âœ… Ejecutar comandos de sistema (git, npm, pip, etc.)
- âœ… Revisar bases de datos y logs
- âœ… Hacer cambios arquitectÃ³nicos cuando sea necesario
- âœ… Refactorizar cÃ³digo sin preguntar previamente
- âœ… Eliminar cÃ³digo muerto o duplicado
- âœ… Crear y modificar tests

**NO es necesario pedir permiso antes de**:
- Revisar o explorar el cÃ³digo
- Ejecutar tests o anÃ¡lisis
- Hacer bÃºsquedas exhaustivas
- Instalar herramientas necesarias
- Corregir bugs evidentes
- Mejorar logging o debugging

**Solo preguntar cuando**:
- Hay mÃºltiples enfoques arquitectÃ³nicos vÃ¡lidos con trade-offs importantes
- Se va a eliminar funcionalidad existente (no cÃ³digo muerto)
- Se necesita aclarar requisitos del usuario

---

## Setup RÃ¡pido (Nueva MÃ¡quina)

**IMPORTANTE**: Solo copiar la carpeta del proyecto. Los modelos estÃ¡n incluidos localmente.

```bash
# 1. Crear entorno virtual
cd tfm
python3.11 -m venv .venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows

# 2. Instalar dependencias
pip install -e ".[dev]"

# 3. Instalar/Configurar Ollama para LLM local
python scripts/setup_ollama.py

# 4. Verificar (NO requiere internet)
python scripts/verify_environment.py
# o
narrative-assistant verify
```

**No se necesita `spacy download`** - el modelo estÃ¡ en `models/spacy/`.
**Ollama se instala y configura con** `setup_ollama.py` - descarga modelos locales automÃ¡ticamente.

---

## Proyecto

**Asistente de CorrecciÃ³n Narrativa** - Herramienta de asistencia a correctores literarios profesionales para detectar inconsistencias en manuscritos de ficciÃ³n.

### Stack TecnolÃ³gico
- **Python** 3.11+
- **NLP**: spaCy (es_core_news_lg), sentence-transformers
- **LLM Local**: Ollama (llama3.2, mistral, qwen2.5)
- **GPU**: PyTorch (CUDA/MPS/CPU auto-detect)
- **DB**: SQLite con WAL mode
- **Formatos**: DOCX (prioritario), TXT, MD, PDF, EPUB

### Requisito de Red
- **Ãšnico acceso a internet**: Sistema de licencias (verificaciÃ³n) e instalaciÃ³n inicial de Ollama
- **Modelos NLP y LLM**: 100% offline desde `models/` y Ollama local

---

## Modelos Locales

Los modelos se almacenan en el proyecto para funcionamiento offline:

```
tfm/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ spacy/
â”‚   â”‚   â””â”€â”€ es_core_news_lg/     # Modelo spaCy espaÃ±ol (~500 MB)
â”‚   â””â”€â”€ embeddings/
â”‚       â””â”€â”€ paraphrase-multilingual-MiniLM-L12-v2/  # sentence-transformers (~500 MB)
```

### Primera vez: Descargar modelos
Si `models/` no existe (primera instalaciÃ³n o proyecto nuevo):
```bash
python scripts/download_models.py
```
Esto descarga los modelos (~1 GB total) y los guarda en `models/`.

### ConfiguraciÃ³n automÃ¡tica
El sistema busca modelos en este orden:
1. `./models/` (proyecto local) - **PREFERIDO**
2. `~/.narrative_assistant/models/` (usuario)
3. Cache de HuggingFace/spaCy (requiere internet)

### Variables de entorno para modelos locales
```bash
# Opcional - por defecto usa ./models/
NA_SPACY_MODEL_PATH=./models/spacy/es_core_news_lg
NA_EMBEDDINGS_MODEL_PATH=./models/embeddings/paraphrase-multilingual-MiniLM-L12-v2
```

---

## Ollama / LLM Local

Ollama proporciona anÃ¡lisis semÃ¡ntico avanzado mediante modelos LLM que corren 100% localmente.

### InstalaciÃ³n
```bash
# AutomÃ¡tica (recomendado)
python scripts/setup_ollama.py

# Manual - Windows
# Descargar desde https://ollama.com/download

# Manual - Linux
curl -fsSL https://ollama.com/install.sh | sh

# Manual - macOS
brew install ollama
```

### Modelos disponibles

| Modelo | TamaÃ±o | Velocidad | Calidad | Notas |
|--------|--------|-----------|---------|-------|
| `llama3.2` | 3B | RÃ¡pido | Buena | **Default**, funciona en CPU |
| `mistral` | 7B | Media | Alta | Mejor razonamiento |
| `qwen2.5` | 7B | Media | Alta | Excelente para espaÃ±ol |
| `gemma2` | 9B | Lento | Muy alta | Requiere mÃ¡s recursos |

### Descargar modelos
```bash
ollama pull llama3.2     # Recomendado (~2 GB)
ollama pull qwen2.5      # Opcional, mejor espaÃ±ol (~4 GB)
ollama pull mistral      # Opcional, mayor calidad (~4 GB)
```

### Iniciar servicio
```bash
ollama serve  # Corre en localhost:11434
```

### Sistema Multi-Modelo (VotaciÃ³n)
El anÃ¡lisis de comportamiento de personajes puede usar mÃºltiples mÃ©todos:

1. **Modelos LLM** (llama3.2, mistral, qwen2.5, gemma2)
2. **Reglas y heurÃ­sticas** (rule_based) - Siempre disponible
3. **Embeddings semÃ¡nticos** (embeddings) - Similitud vectorial

ConfiguraciÃ³n en Settings:
- **MÃ©todos habilitados**: SelecciÃ³n mÃºltiple de mÃ©todos
- **Confianza mÃ­nima**: Umbral para mostrar expectativas
- **Consenso mÃ­nimo**: Porcentaje de mÃ©todos que deben coincidir

### Variables de entorno LLM
```bash
NA_LLM_BACKEND=ollama              # Backend: ollama, transformers, none
NA_OLLAMA_HOST=http://localhost:11434  # URL del servidor Ollama
NA_OLLAMA_MODEL=llama3.2           # Modelo por defecto
```

---

## Convenciones de CÃ³digo

### Python
- **Estilo**: PEP 8, Black formatter, isort
- **Type hints**: Obligatorios en funciones pÃºblicas
- **Docstrings**: Google style en espaÃ±ol
- **Imports**: Relativos dentro del paquete (`from ..core.config import ...`)

### Patrones ArquitectÃ³nicos

#### Result Pattern
Para operaciones que pueden tener Ã©xitos parciales:
```python
from narrative_assistant.core import Result

def process(data) -> Result[OutputType]:
    if error_condition:
        return Result.failure(SomeError(...))
    return Result.success(output)
```

#### Singleton Thread-Safe
Todos los singletons usan double-checked locking:
```python
import threading

_lock = threading.Lock()
_instance = None

def get_instance():
    global _instance
    if _instance is None:
        with _lock:
            if _instance is None:
                _instance = create_instance()
    return _instance
```

#### ValidaciÃ³n de Archivos
Los parsers SIEMPRE validan antes de abrir:
```python
def parse(self, path: Path) -> Result[RawDocument]:
    validation = self.validate_file(path)  # Path traversal, size, extension
    if validation.is_failure:
        return validation
    # ... continuar
```

---

## Estructura de MÃ³dulos

```
src/narrative_assistant/
â”œâ”€â”€ core/           # Infraestructura base
â”‚   â”œâ”€â”€ config.py   # ConfiguraciÃ³n centralizada (GPUConfig, NLPConfig, etc.)
â”‚   â”œâ”€â”€ device.py   # DetecciÃ³n GPU (CUDA, MPS, CPU)
â”‚   â”œâ”€â”€ errors.py   # JerarquÃ­a de errores (NarrativeError, severity levels)
â”‚   â””â”€â”€ result.py   # Result[T] pattern
â”‚
â”œâ”€â”€ persistence/    # Estado y base de datos
â”‚   â”œâ”€â”€ database.py           # SQLite, transacciones
â”‚   â”œâ”€â”€ project.py            # Proyecto = un manuscrito
â”‚   â”œâ”€â”€ document_fingerprint.py  # SHA-256 + n-gram Jaccard
â”‚   â”œâ”€â”€ session.py            # SesiÃ³n de trabajo del revisor
â”‚   â””â”€â”€ history.py            # Historial de cambios (undo/redo)
â”‚
â”œâ”€â”€ parsers/        # Lectura de documentos
â”‚   â”œâ”€â”€ base.py              # DocumentParser ABC, RawDocument
â”‚   â”œâ”€â”€ docx_parser.py       # Word (.docx)
â”‚   â”œâ”€â”€ txt_parser.py        # Texto plano + Markdown
â”‚   â””â”€â”€ sanitization.py      # InputSanitizer, validate_file_path
â”‚
â””â”€â”€ nlp/            # Procesamiento NLP
    â”œâ”€â”€ spacy_gpu.py    # setup_spacy_gpu(), load_spacy_model()
    â”œâ”€â”€ embeddings.py   # EmbeddingsModel con fallback OOM
    â””â”€â”€ chunking.py     # TextChunker para docs grandes
```

---

## Comandos de Desarrollo

```bash
# Verificar entorno (offline)
narrative-assistant verify

# Info del sistema
narrative-assistant info

# Analizar documento
narrative-assistant analyze documento.docx

# Tests
pytest -v

# Formateo
black src/ && isort src/

# Type checking
mypy src/
```

---

## Variables de Entorno

| Variable | Valores | Default | DescripciÃ³n |
|----------|---------|---------|-------------|
| `NA_DEVICE` | auto, cuda, mps, cpu | auto | Dispositivo preferido |
| `NA_SPACY_GPU` | true, false | true | Habilitar GPU para spaCy |
| `NA_EMBEDDINGS_GPU` | true, false | true | Habilitar GPU para embeddings |
| `NA_BATCH_SIZE_GPU` | int | 64 | Batch size en GPU |
| `NA_BATCH_SIZE_CPU` | int | 16 | Batch size en CPU |
| `NA_LOG_LEVEL` | DEBUG, INFO, WARNING, ERROR | INFO | Nivel de logging |
| `NA_DATA_DIR` | path | ~/.narrative_assistant | Directorio de datos |
| `NA_SPACY_MODEL_PATH` | path | ./models/spacy/es_core_news_lg | Modelo spaCy local |
| `NA_EMBEDDINGS_MODEL_PATH` | path | ./models/embeddings/... | Modelo embeddings local |

---

## Errores Comunes y Soluciones

### GPU OOM
El sistema tiene fallback automÃ¡tico a CPU con batch reducido. Si persiste:
```bash
export NA_BATCH_SIZE_GPU=32  # Reducir
# o
export NA_DEVICE=cpu  # Forzar CPU
```

### Modelo no encontrado
Verificar que `models/` estÃ© copiado con el proyecto:
```bash
ls -la models/spacy/es_core_news_lg/
ls -la models/embeddings/
```

### Import errors
Verificar que el paquete estÃ¡ instalado en modo editable:
```bash
pip install -e .
```

---

## Seguridad - Aislamiento de Manuscritos

**CRÃTICO**: Los manuscritos NUNCA deben salir de la mÃ¡quina del usuario.

### Reglas de seguridad obligatorias

1. **Sin acceso a internet** excepto verificaciÃ³n de licencias
2. **Modelos NLP solo locales** - fallar si no estÃ¡n en `models/`
3. **Sin telemetrÃ­a** ni analytics de ningÃºn tipo
4. **Sin auto-updates** de modelos o dependencias

### Variables de entorno forzadas
```python
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"
```

### Al generar cÃ³digo - PROHIBIDO:
- âŒ `requests`, `urllib`, `httpx`, `aiohttp`
- âŒ Cualquier llamada HTTP/HTTPS (excepto licencias)
- âŒ Enviar datos a servicios externos
- âŒ Descargas automÃ¡ticas de modelos
- âŒ Analytics, telemetrÃ­a, logging remoto

Ver: [docs/02-architecture/SECURITY.md](docs/02-architecture/SECURITY.md)

---

## Notas para Claude

1. **Idioma**: El cÃ³digo estÃ¡ en inglÃ©s, docstrings y comentarios en espaÃ±ol
2. **Tests**: AÃºn no implementados, priorizar implementaciÃ³n
3. **UI**: Vue 3 + PrimeVue (frontend), FastAPI (api-server)
4. **LLM Integration**: Ollama para anÃ¡lisis semÃ¡ntico local (100% offline)
5. **Offline**: Todos los modelos (NLP y LLM) son locales, no requieren internet

### Al generar cÃ³digo:
- Usar type hints siempre
- Seguir Result pattern para operaciones fallibles
- Validar inputs (especialmente paths de archivos)
- AÃ±adir logging apropiado
- Considerar thread-safety en singletons
- **NO aÃ±adir cÃ³digo que requiera internet** (excepto licencias)
- **Verificar que no hay filtraciones de datos**
