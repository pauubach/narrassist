# Estudio: DetecciÃ³n de Redundancia SemÃ¡ntica

**Fecha**: 4 de febrero de 2026
**Objetivo**: Analizar las mejores prÃ¡cticas para implementar detecciÃ³n de redundancia semÃ¡ntica de forma eficiente
**Estado**: Estudio completado - Pendiente de implementaciÃ³n

---

## 1. Resumen Ejecutivo

La detecciÃ³n de redundancia semÃ¡ntica busca identificar contenido que se repite conceptualmente aunque estÃ© escrito con palabras diferentes. Este estudio analiza las opciones de implementaciÃ³n, trade-offs de rendimiento, y estrategias de optimizaciÃ³n para reducir la complejidad computacional de O(nÂ²) a complejidades sublineales.

### Conclusiones Principales

| Aspecto | RecomendaciÃ³n |
|---------|---------------|
| **Algoritmo base** | FAISS + Sentence Transformers |
| **Complejidad optimizada** | O(n log n) con Ã­ndices ANN |
| **Umbral recomendado** | 0.80-0.85 (configurable) |
| **Chunking** | Nivel oraciÃ³n con agrupaciÃ³n semÃ¡ntica |
| **Modo default** | Deshabilitado (opt-in por usuario) |
| **Requisitos mÃ­nimos** | 4GB RAM, GPU opcional pero recomendada |

---

## 2. AnÃ¡lisis del Problema

### 2.1 Casos de Uso

1. **Reescritura involuntaria**: El autor escribiÃ³ algo que ya estaba escrito antes pero con otras palabras
2. **Acciones repetidas**: Un personaje hace algo que ya habÃ­a hecho antes (mismo evento, distinta narraciÃ³n)
3. **Insistencia temÃ¡tica excesiva**: Se repite el mismo tema demasiadas veces

### 2.2 Complejidad Naive

```
Comparar todas las oraciones entre sÃ­:
- n oraciones â†’ n(n-1)/2 comparaciones
- Documento de 10,000 oraciones â†’ ~50 millones de comparaciones
- Tiempo estimado (CPU): 30-60 minutos
- Tiempo estimado (GPU): 2-5 minutos
```

### 2.3 Objetivo de OptimizaciÃ³n

Reducir la complejidad de **O(nÂ²)** a **O(n log n)** o mejor, manteniendo alta precisiÃ³n.

---

## 3. Estrategias de OptimizaciÃ³n

### 3.1 Approximate Nearest Neighbors (ANN)

En lugar de comparar todos los vectores uno a uno, los algoritmos ANN organizan los vectores en estructuras de datos que permiten bÃºsquedas eficientes.

#### Comparativa de Algoritmos ANN

| Algoritmo | Complejidad BÃºsqueda | Memoria | PrecisiÃ³n | Mejor Para |
|-----------|---------------------|---------|-----------|------------|
| **FAISS IVF** | O(âˆšn) | Media | Alta | Datasets medianos (100K-10M) |
| **FAISS HNSW** | O(log n) | Alta | Muy alta | Alta precisiÃ³n requerida |
| **LSH (MinHash)** | O(1) amortizado | Baja | Media | Datasets muy grandes (>10M) |
| **ScaNN** | O(log n) | Media | Alta | Balance velocidad/precisiÃ³n |

**RecomendaciÃ³n para Narrative Assistant**: FAISS con Ã­ndice IVF (Inverted File) para balance Ã³ptimo.

### 3.2 Locality Sensitive Hashing (LSH)

LSH agrupa elementos similares en los mismos "buckets" con alta probabilidad, reduciendo drÃ¡sticamente el espacio de bÃºsqueda.

```python
# PseudocÃ³digo conceptual
class LSHIndex:
    def __init__(self, num_bands=20, rows_per_band=5):
        self.bands = num_bands
        self.rows = rows_per_band
        self.buckets = [defaultdict(list) for _ in range(num_bands)]

    def add(self, doc_id, minhash_signature):
        for band_idx in range(self.bands):
            start = band_idx * self.rows
            band_hash = hash(tuple(minhash_signature[start:start+self.rows]))
            self.buckets[band_idx][band_hash].append(doc_id)

    def query(self, minhash_signature):
        candidates = set()
        for band_idx in range(self.bands):
            start = band_idx * self.rows
            band_hash = hash(tuple(minhash_signature[start:start+self.rows]))
            candidates.update(self.buckets[band_idx][band_hash])
        return candidates
```

**Ventajas**:
- Muy eficiente en memoria (~11 GB para millones de documentos vs ~200 GB con MinHash tradicional)
- Escalable a datasets de billones de tokens
- Procesamiento incremental posible

**Desventajas**:
- Menor precisiÃ³n que embeddings semÃ¡nticos puros
- Requiere tuning de parÃ¡metros (bands, rows)

### 3.3 Enfoque HÃ­brido (Recomendado)

Combinar LSH para filtrado inicial + embeddings para verificaciÃ³n:

```
1. Fase 1 (Filtrado): LSH para identificar candidatos (~O(n))
2. Fase 2 (VerificaciÃ³n): Cosine similarity solo entre candidatos (~O(kÂ²) donde k << n)
```

**Benchmarks de referencia**:
- SemHash: 1.8M textos en ~83 segundos (CPU)
- LSHBloom: 12Ã— mÃ¡s rÃ¡pido que MinHashLSH tradicional
- FAISS GPU: 75Ã— mÃ¡s rÃ¡pido que CPU (Tesla T4)

---

## 4. Estrategias de Chunking

### 4.1 Comparativa de Niveles

| Nivel | Pros | Contras | Uso Recomendado |
|-------|------|---------|-----------------|
| **OraciÃ³n** | Alta granularidad, detecta duplicados exactos | Muchos chunks, mÃ¡s comparaciones | DetecciÃ³n de repeticiones textuales |
| **PÃ¡rrafo** | Contexto mÃ¡s rico, menos chunks | Puede perder duplicados parciales | DetecciÃ³n de ideas repetidas |
| **SemÃ¡ntico** | Agrupa por significado | Computacionalmente costoso | MÃ¡xima calidad, menor volumen |

### 4.2 Chunking SemÃ¡ntico Adaptativo

```python
# PseudocÃ³digo para chunking semÃ¡ntico
def semantic_chunk(sentences, threshold=0.75):
    embeddings = model.encode(sentences)
    chunks = []
    current_chunk = [sentences[0]]

    for i in range(1, len(sentences)):
        similarity = cosine_similarity(embeddings[i-1], embeddings[i])

        if similarity < threshold:  # Cambio de tema detectado
            chunks.append(current_chunk)
            current_chunk = []

        current_chunk.append(sentences[i])

    chunks.append(current_chunk)
    return chunks
```

**RecomendaciÃ³n**: Usar chunking a nivel de oraciÃ³n con post-agrupaciÃ³n semÃ¡ntica opcional.

---

## 5. Umbrales de Similitud

### 5.1 Rangos Recomendados

| Umbral | Comportamiento | Uso |
|--------|----------------|-----|
| **0.95+** | Solo casi-idÃ©nticos | DetecciÃ³n de copias exactas |
| **0.85-0.95** | Alta similitud | Balance recomendado |
| **0.75-0.85** | Similitud moderada | DetecciÃ³n agresiva |
| **<0.75** | Similitud baja | Muchos falsos positivos |

### 5.2 CalibraciÃ³n por Dominio

No existe un umbral universal. Se recomienda:

1. **Fase de calibraciÃ³n**: Muestra de 50-100 pares evaluados manualmente
2. **MÃ©tricas objetivo**: Maximizar F1-score en el conjunto de validaciÃ³n
3. **Umbrales diferenciados**: Diferentes umbrales para diferentes tipos de redundancia

```python
# Ejemplo de calibraciÃ³n automÃ¡tica
def find_optimal_threshold(pairs, labels, model):
    embeddings = model.encode([p[0] for p in pairs] + [p[1] for p in pairs])
    similarities = [cosine_similarity(e1, e2) for e1, e2 in ...]

    best_f1, best_threshold = 0, 0.8
    for threshold in np.arange(0.5, 0.99, 0.01):
        predictions = [1 if sim >= threshold else 0 for sim in similarities]
        f1 = f1_score(labels, predictions)
        if f1 > best_f1:
            best_f1, best_threshold = f1, threshold

    return best_threshold, best_f1
```

---

## 6. Arquitectura Propuesta

### 6.1 Diagrama de Flujo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DETECCIÃ“N DE REDUNDANCIA SEMÃNTICA               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Documento   â”‚â”€â”€â”€â–¶â”‚   Chunking   â”‚â”€â”€â”€â–¶â”‚  Embeddings  â”‚          â”‚
â”‚  â”‚   (texto)    â”‚    â”‚  (oraciones) â”‚    â”‚ (vectores)   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                 â”‚                   â”‚
â”‚                                                 â–¼                   â”‚
â”‚                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚                             â”‚      FAISS Index (IVF)          â”‚     â”‚
â”‚                             â”‚   - Clustering automÃ¡tico       â”‚     â”‚
â”‚                             â”‚   - BÃºsqueda O(âˆšn)              â”‚     â”‚
â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                 â”‚                   â”‚
â”‚                                                 â–¼                   â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                      â”‚     Candidatos (k vecinos cercanos)   â”‚      â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                 â”‚                   â”‚
â”‚                                                 â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Filtrado   â”‚â—€â”€â”€â”€â”‚   Cosine     â”‚â—€â”€â”€â”€â”‚  VerificaciÃ³nâ”‚          â”‚
â”‚  â”‚  (umbral)    â”‚    â”‚  Similarity  â”‚    â”‚   (pares)    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚         â”‚                                                           â”‚
â”‚         â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚              Reporte de Redundancias                      â”‚      â”‚
â”‚  â”‚  - Pares de oraciones/pÃ¡rrafos similares                  â”‚      â”‚
â”‚  â”‚  - CapÃ­tulos y posiciones                                 â”‚      â”‚
â”‚  â”‚  - Score de similitud                                     â”‚      â”‚
â”‚  â”‚  - ClasificaciÃ³n (textual / temÃ¡tica / acciÃ³n)            â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Componentes

```python
# Estructura propuesta de clases

@dataclass
class SemanticDuplicate:
    """Par de textos semÃ¡nticamente similares."""
    text1: str
    text2: str
    chapter1: int
    chapter2: int
    position1: int
    position2: int
    similarity: float
    duplicate_type: str  # "textual", "thematic", "action"

@dataclass
class RedundancyReport:
    """Reporte de redundancias detectadas."""
    duplicates: list[SemanticDuplicate]
    sentences_analyzed: int
    clusters_found: int
    processing_time_seconds: float

class SemanticRedundancyDetector:
    """Detector de redundancia semÃ¡ntica optimizado."""

    def __init__(
        self,
        model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
        similarity_threshold: float = 0.85,
        index_type: str = "IVF",  # IVF, HNSW, Flat
        use_gpu: bool = True,
        min_sentence_length: int = 10,
    ):
        ...

    def build_index(self, sentences: list[str]) -> None:
        """Construye Ã­ndice FAISS de embeddings."""
        ...

    def find_duplicates(
        self,
        chapters: list[dict],
        k_neighbors: int = 10,
    ) -> Result[RedundancyReport]:
        """Detecta duplicados semÃ¡nticos en capÃ­tulos."""
        ...

    def detect_thematic_overemphasis(
        self,
        chapters: list[dict],
        theme_threshold: float = 0.7,
    ) -> list[ThematicCluster]:
        """Detecta temas repetidos excesivamente."""
        ...
```

---

## 7. ConfiguraciÃ³n de Usuario

### 7.1 Opciones Recomendadas

```typescript
// Settings del usuario
interface SemanticRedundancySettings {
  // HabilitaciÃ³n
  enabled: boolean;              // Default: false (opt-in)

  // PrecisiÃ³n vs Velocidad
  mode: "fast" | "balanced" | "thorough";  // Default: balanced

  // Umbrales
  similarityThreshold: number;   // Default: 0.85, Range: [0.70, 0.95]
  minSentenceLength: number;     // Default: 20 caracteres

  // Tipos de detecciÃ³n
  detectTextualDuplicates: boolean;    // Default: true
  detectThematicOveremphasis: boolean; // Default: true
  detectRepeatedActions: boolean;      // Default: false (mÃ¡s lento)

  // Rendimiento
  useGpu: boolean;               // Default: auto-detect
  maxSentencesPerAnalysis: number;  // Default: 10000
}
```

### 7.2 Modos de OperaciÃ³n

| Modo | Algoritmo | Tiempo (10K oraciones) | PrecisiÃ³n |
|------|-----------|------------------------|-----------|
| **Fast** | LSH + top-100 candidatos | ~5 seg | 85% |
| **Balanced** | FAISS IVF + top-500 | ~30 seg | 95% |
| **Thorough** | FAISS Flat (exhaustivo) | ~5 min | 99% |

---

## 8. Requisitos de Recursos

### 8.1 Memoria

| Componente | Memoria Estimada |
|------------|-----------------|
| Embeddings (10K oraciones Ã— 384 dim) | ~15 MB |
| Ãndice FAISS IVF | ~20 MB |
| Modelo sentence-transformers | ~500 MB |
| **Total mÃ­nimo** | **~600 MB** |
| **Recomendado** | **2-4 GB** |

### 8.2 Tiempo de Procesamiento

| Documento | CPU (i7) | GPU (RTX 3060) |
|-----------|----------|----------------|
| 1,000 oraciones | 5 seg | <1 seg |
| 10,000 oraciones | 45 seg | 3 seg |
| 50,000 oraciones | 4 min | 15 seg |
| 100,000 oraciones | 15 min | 1 min |

### 8.3 RecomendaciÃ³n de Hardware

- **MÃ­nimo**: CPU 4 cores, 4 GB RAM
- **Recomendado**: GPU con 4+ GB VRAM, 8 GB RAM
- **Ã“ptimo**: GPU con 8+ GB VRAM, 16 GB RAM

---

## 9. Estrategia Incremental

Para documentos que se editan frecuentemente, mantener un Ã­ndice persistente:

```python
class IncrementalRedundancyIndex:
    """Ãndice persistente para detecciÃ³n incremental."""

    def __init__(self, project_id: int):
        self.index_path = f"~/.narrative_assistant/indexes/{project_id}_semantic.faiss"
        self.metadata_path = f"~/.narrative_assistant/indexes/{project_id}_metadata.json"

    def add_sentences(self, new_sentences: list[str], chapter: int) -> list[SemanticDuplicate]:
        """
        AÃ±ade nuevas oraciones y detecta duplicados contra el Ã­ndice existente.
        Solo compara nuevas vs existentes (no nÂ² completo).
        """
        # 1. Generar embeddings de nuevas oraciones
        new_embeddings = self.model.encode(new_sentences)

        # 2. Buscar vecinos cercanos en Ã­ndice existente
        distances, indices = self.index.search(new_embeddings, k=10)

        # 3. Filtrar por umbral
        duplicates = []
        for i, (dists, idxs) in enumerate(zip(distances, indices)):
            for dist, idx in zip(dists, idxs):
                if dist < self.threshold:
                    duplicates.append(...)

        # 4. AÃ±adir nuevas oraciones al Ã­ndice
        self.index.add(new_embeddings)

        return duplicates

    def rebuild_full(self, chapters: list[dict]) -> None:
        """Reconstruye Ã­ndice completo (ejecutar ocasionalmente)."""
        ...
```

**Ventaja**: AnÃ¡lisis de nuevos capÃ­tulos en O(m Ã— log n) donde m = oraciones nuevas, n = total.

---

## 10. MitigaciÃ³n de Falsos Positivos

### 10.1 Estrategias

1. **Filtro de frases comunes**: Excluir frases muy frecuentes en espaÃ±ol
   ```python
   COMMON_PHRASES = {
       "dijo que", "se levantÃ³", "mirÃ³ a", "pensÃ³ en",
       "al dÃ­a siguiente", "en ese momento", ...
   }
   ```

2. **Filtro de diÃ¡logo**: Los diÃ¡logos cortos no deberÃ­an marcarse como duplicados
   ```python
   def is_dialogue(text: str) -> bool:
       return text.startswith("â€”") or text.startswith('"')
   ```

3. **Contexto de capÃ­tulo**: Duplicados en el mismo capÃ­tulo tienen diferente peso
   ```python
   if chapter1 == chapter2:
       score *= 0.8  # Menos relevante si es mismo capÃ­tulo
   ```

4. **VerificaciÃ³n semÃ¡ntica secundaria**: Usar LLM local para verificar casos borderline
   ```python
   if 0.80 <= similarity <= 0.90:
       # Verificar con Ollama
       is_truly_duplicate = verify_with_llm(text1, text2)
   ```

### 10.2 CategorizaciÃ³n de Duplicados

| CategorÃ­a | DescripciÃ³n | Umbral Sugerido |
|-----------|-------------|-----------------|
| **Exacto** | Mismo texto o muy similar | >0.95 |
| **ParÃ¡frasis** | Misma idea, diferentes palabras | 0.85-0.95 |
| **TemÃ¡tico** | Mismo tema general | 0.75-0.85 |
| **Relacionado** | Conceptos relacionados | <0.75 (ignorar) |

---

## 11. IntegraciÃ³n con Sistema Existente

### 11.1 IntegraciÃ³n con AlertEngine

```python
# En AlertEngine
def create_from_semantic_duplicate(
    self,
    project_id: int,
    duplicate: SemanticDuplicate,
) -> Result[Alert]:
    """Crea alerta desde duplicado semÃ¡ntico."""
    return self.create_alert(
        project_id=project_id,
        category=AlertCategory.STRUCTURE,  # O nueva categorÃ­a REDUNDANCY
        severity=self._map_similarity_to_severity(duplicate.similarity),
        alert_type=f"semantic_{duplicate.duplicate_type}",
        title=self._get_duplicate_title(duplicate),
        description=f"Contenido similar encontrado en capÃ­tulos {duplicate.chapter1} y {duplicate.chapter2}",
        suggestion="Considerar eliminar o reformular uno de los pasajes",
        ...
    )
```

### 11.2 API Endpoints

```python
@router.get("/api/projects/{project_id}/semantic-redundancy")
async def detect_semantic_redundancy(
    project_id: str,
    mode: str = Query("balanced", enum=["fast", "balanced", "thorough"]),
    threshold: float = Query(0.85, ge=0.70, le=0.95),
    max_results: int = Query(50, ge=10, le=200),
    create_alerts: bool = Query(False),
) -> ApiResponse:
    """Detecta redundancia semÃ¡ntica en el proyecto."""
    ...

@router.get("/api/projects/{project_id}/semantic-redundancy/preview")
async def preview_redundancy_detection(
    project_id: str,
    chapter_number: int,
) -> ApiResponse:
    """Preview rÃ¡pido para un capÃ­tulo especÃ­fico."""
    ...
```

### 11.3 UI Propuesta

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redundancia SemÃ¡ntica                              [Analizar]  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âš™ï¸ ConfiguraciÃ³n                                               â”‚
â”‚  â”œâ”€ Modo: [Balanced â–¼]                                         â”‚
â”‚  â”œâ”€ Umbral: [0.85 â”€â”€â”€â”€â—â”€â”€â”€â”€]                                   â”‚
â”‚  â””â”€ Tipos: [âœ“] Textual  [âœ“] TemÃ¡tica  [ ] Acciones             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“Š Resultados (12 encontrados)                                 â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 92% similitud - CapÃ­tulos 3 y 7                         â”‚   â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   â”‚
â”‚  â”‚ Cap 3: "MarÃ­a observÃ³ el paisaje con melancolÃ­a,       â”‚   â”‚
â”‚  â”‚        recordando los dÃ­as felices de su juventud."    â”‚   â”‚
â”‚  â”‚ Cap 7: "Con tristeza, MarÃ­a contemplÃ³ el horizonte,    â”‚   â”‚
â”‚  â”‚        aÃ±orando los tiempos de su juventud feliz."     â”‚   â”‚
â”‚  â”‚ [Ir a Cap 3] [Ir a Cap 7] [Ignorar]                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  ...                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 12. Plan de ImplementaciÃ³n Sugerido

### Fase 1: Core (1-2 semanas)
- [ ] `SemanticRedundancyDetector` con FAISS
- [ ] Tests unitarios
- [ ] IntegraciÃ³n con embeddings existentes

### Fase 2: OptimizaciÃ³n (1 semana)
- [ ] Ãndice incremental
- [ ] Filtros de falsos positivos
- [ ] CalibraciÃ³n de umbrales

### Fase 3: IntegraciÃ³n (1 semana)
- [ ] Endpoint API
- [ ] IntegraciÃ³n con AlertEngine
- [ ] ConfiguraciÃ³n en Settings

### Fase 4: UI (1 semana)
- [ ] Tab o panel en workspace
- [ ] VisualizaciÃ³n de resultados
- [ ] NavegaciÃ³n a pasajes

---

## 13. Fuentes y Referencias

### ArtÃ­culos y DocumentaciÃ³n

- [Billion-scale semantic similarity search with FAISS+SBERT](https://towardsdatascience.com/billion-scale-semantic-similarity-search-with-faiss-sbert-c845614962e2/)
- [Master Semantic Search at Scale](https://towardsdatascience.com/master-semantic-search-at-scale-index-millions-of-documents-with-lightning-fast-inference-times-fa395e4efd88/)
- [Semantic search with FAISS - Hugging Face](https://huggingface.co/learn/llm-course/en/chapter5/6)
- [SemDeDup: Data-efficient learning through semantic deduplication](https://arxiv.org/abs/2303.09540)
- [Large-scale Near-deduplication Behind BigCode](https://huggingface.co/blog/dedup)

### Herramientas y LibrerÃ­as

- [FAISS - Facebook AI Similarity Search](https://github.com/facebookresearch/faiss)
- [Sentence Transformers](https://sbert.net/)
- [SemHash - Fast Multimodal Semantic Deduplication](https://github.com/MinishLab/semhash)
- [Datasketch - Probabilistic Data Structures](https://github.com/ekzhu/datasketch)
- [NVIDIA NeMo Curator - Semantic Deduplication](https://docs.nvidia.com/nemo-framework/user-guide/24.09/datacuration/semdedup.html)

### Chunking y OptimizaciÃ³n

- [Chunking Strategies for LLM Applications - Pinecone](https://www.pinecone.io/learn/chunking-strategies/)
- [Semantic Chunking for RAG](https://medium.com/the-ai-forum/semantic-chunking-for-rag-f4733025d5f5)
- [How to split text based on semantic similarity - LangChain](https://python.langchain.com/docs/how_to/semantic-chunker/)

### Umbrales y CalibraciÃ³n

- [How do you tune similarity thresholds to reduce false positives?](https://milvus.io/ai-quick-reference/how-do-you-tune-similarity-thresholds-to-reduce-false-positives)
- [Sentence Transformers Evaluation](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html)

### LSH y Algoritmos ANN

- [Locality-sensitive hashing - Wikipedia](https://en.wikipedia.org/wiki/Locality-sensitive_hashing)
- [Near-duplicate Detection with LSH and Datasketch](https://yorko.github.io/2023/practical-near-dup-detection/)
- [MinHash LSH in Milvus](https://milvus.io/blog/minhash-lsh-in-milvus-the-secret-weapon-for-fighting-duplicates-in-llm-training-data.md)

---

## 14. Conclusiones

### Viabilidad: âœ… Alta

La detecciÃ³n de redundancia semÃ¡ntica es tÃ©cnicamente viable con las herramientas actuales. Las optimizaciones con FAISS + ANN reducen la complejidad de O(nÂ²) a O(n log n), haciÃ©ndolo prÃ¡ctico para documentos de hasta ~100K oraciones.

### Recomendaciones Clave

1. **Implementar como opt-in**: Deshabilitado por defecto debido al costo computacional
2. **Usar enfoque hÃ­brido**: FAISS IVF para balance velocidad/precisiÃ³n
3. **Ofrecer modos**: Fast/Balanced/Thorough para diferentes necesidades
4. **Calibrar umbrales**: 0.85 como default, configurable por usuario
5. **Filtrar falsos positivos**: Excluir diÃ¡logos cortos y frases comunes
6. **Ãndice incremental**: Para anÃ¡lisis eficiente de cambios

### Siguiente Paso

Cuando se decida implementar, comenzar por la Fase 1 (Core) con un prototipo simple que use FAISS Flat para validar el concepto antes de optimizar.
