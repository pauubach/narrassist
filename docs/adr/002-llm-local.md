# ADR-002: LLM Local con Ollama para AnÃ¡lisis SemÃ¡ntico

## Estado

**Aceptada** â€” 2026-01-15 (Sprint S1)

## Contexto

El anÃ¡lisis NLP tradicional (spaCy, reglas heurÃ­sticas) tiene limitaciones para:
- DetecciÃ³n de comportamiento fuera de personaje (OOC)
- AnÃ¡lisis de tono emocional y registro de habla
- Inferencia de intenciones y motivaciones
- DetecciÃ³n de contradicciones narrativas sutiles
- ValidaciÃ³n de correferencias ambiguas

Los LLMs pueden mejorar significativamente la precisiÃ³n, pero:
1. **Privacidad**: APIs externas (OpenAI, Anthropic) requieren enviar texto del manuscrito â†’ **INACEPTABLE**
2. **Costo**: Procesar novelas de 100k palabras con APIs de pago â†’ miles de USD por anÃ¡lisis
3. **Latencia**: APIs remotas aÃ±aden latencia de red
4. **Disponibilidad**: Requieren conexiÃ³n a internet constante

Alternativas consideradas:

| OpciÃ³n | Privacidad | Costo | Performance | Disponibilidad |
|--------|------------|-------|-------------|----------------|
| **OpenAI API** | âŒ EnvÃ­a datos | ğŸ’°ğŸ’°ğŸ’° Alto | âš¡ RÃ¡pido | â˜ï¸ Online |
| **Anthropic API** | âŒ EnvÃ­a datos | ğŸ’°ğŸ’°ğŸ’° Alto | âš¡ RÃ¡pido | â˜ï¸ Online |
| **Transformers local** | âœ… Privado | âœ… Gratis | ğŸŒ Lento | ğŸ“´ Offline |
| **Ollama** | âœ… Privado | âœ… Gratis | âš¡ RÃ¡pido | ğŸ“´ Offline |

## DecisiÃ³n

Usar **Ollama** como runtime local de LLMs con los siguientes modelos:

| Modelo | TamaÃ±o | Uso | Notas |
|--------|--------|-----|-------|
| **llama3.2** | 3B | Default | RÃ¡pido, funciona en CPU |
| **qwen2.5** | 7B | EspaÃ±ol | Mejor para espaÃ±ol |
| **mistral** | 7B | Razonamiento | Mayor calidad de anÃ¡lisis |
| **gemma2** | 9B | Alta precisiÃ³n | Requiere GPU |

**Arquitectura**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Narrative Asst. â”‚
â”‚    (FastAPI)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP localhost:11434
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Ollama      â”‚
â”‚   (Servidor)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Carga modelos .gguf
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ~/.ollama/     â”‚
â”‚   models/       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**VotaciÃ³n multi-modelo**:
- Correferencias: embeddings (30%) + LLM (35%) + morpho (20%) + heuristics (15%)
- AnÃ¡lisis de comportamiento: rule_based + LLM (llama3.2, qwen2.5, mistral)
- Consenso mÃ­nimo configurable en Settings

**ConfiguraciÃ³n**:
```bash
# Variables de entorno
NA_LLM_BACKEND=ollama  # ollama, transformers, none
NA_OLLAMA_HOST=http://localhost:11434
NA_OLLAMA_MODEL=llama3.2  # modelo por defecto

# Fallback
# Si Ollama no disponible â†’ sistema funciona sin LLM (solo heurÃ­sticas)
```

## Consecuencias

### Positivas âœ…

1. **Privacidad absoluta**: Modelos corren 100% localmente, texto nunca sale del PC
2. **Costo cero**: Sin cargos por tokens ni lÃ­mites de uso
3. **Offline-first**: Funciona sin internet despuÃ©s de descargar modelos
4. **Flexibilidad**: MÃºltiples modelos disponibles, usuario elige segÃºn hardware
5. **Comunidad activa**: Ollama tiene 100k+ estrellas en GitHub, modelos constantemente actualizados
6. **FÃ¡cil instalaciÃ³n**: Instalador one-click para Windows/macOS/Linux
7. **GGUF quantization**: Modelos optimizados para correr en hardware modesto

### Negativas âš ï¸

1. **Requisitos de hardware**:
   - MÃ­nimo 8 GB RAM para llama3.2 (3B)
   - Recomendado 16 GB RAM para qwen2.5/mistral (7B)
   - GPU con 4+ GB VRAM mejora velocidad
2. **Descarga inicial**: Modelos ocupan 2-4 GB cada uno
3. **Velocidad variable**:
   - CPU: 5-10 tokens/s (lento pero funcional)
   - GPU: 30-50 tokens/s (rÃ¡pido)
4. **Calidad menor que GPT-4**: Modelos locales son menos capaces que modelos cloud de Ãºltima generaciÃ³n
5. **Setup adicional**: Requiere instalar y configurar Ollama (mitigado con `setup_ollama.py`)

### Mitigaciones

- **Fallback graceful**: Si Ollama no disponible, sistema funciona con mÃ©todos no-LLM
- **InstalaciÃ³n automÃ¡tica**: `python scripts/setup_ollama.py` automatiza la instalaciÃ³n
- **CPU fallback**: Script `start_ollama_cpu.bat` para hardware limitado
- **SelecciÃ³n de modelo**: Usuario elige entre calidad (gemma2) y velocidad (llama3.2)
- **Chunking**: Textos largos se dividen en chunks para no saturar contexto

## Notas de ImplementaciÃ³n

Ver:
- `src/narrative_assistant/llm/ollama_client.py` â€” cliente HTTP para Ollama
- `src/narrative_assistant/llm/prompts.py` â€” prompts con CoT y anti-injection
- `src/narrative_assistant/llm/sanitization.py` â€” sanitizaciÃ³n de inputs
- `src/narrative_assistant/nlp/coreference_resolver.py` â€” votaciÃ³n multi-mÃ©todo
- `scripts/setup_ollama.py` â€” instalaciÃ³n automatizada

**Prompting**:
- Chain-of-Thought (CoT) para razonamiento explÃ­cito
- Self-reflection en detecciÃ³n de contradicciones
- Anti-injection sanitization (protecciÃ³n contra texto malicioso en manuscritos)

## Referencias

- [Ollama](https://ollama.com/) â€” Runtime local de LLMs
- [GGUF Format](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) â€” QuantizaciÃ³n eficiente
- [Qwen 2.5](https://huggingface.co/Qwen/Qwen2.5-7B) â€” Mejor modelo para espaÃ±ol
- Implementado en Sprint S1, mejorado en S5 (prompting avanzado)
