# CI Pipeline - Narrative Assistant
# Ejecuta tests en cada push y PR

name: CI

on:
  push:
    branches: [master, main, develop]
    paths:
      - 'src/**'
      - 'tests/**'
      - 'api-server/**'
      - 'pyproject.toml'
      - 'pytest.ini'
      - '.github/workflows/ci.yml'
  pull_request:
    branches: [master, main]
    paths:
      - 'src/**'
      - 'tests/**'
      - 'api-server/**'
      - 'pyproject.toml'

jobs:
  # Tests unitarios (rápidos, sin dependencias NLP pesadas)
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -e ".[dev]"

      - name: Run unit tests
        run: |
          pytest tests/unit -v --tb=short -x -m "not slow and not heavy" --junitxml=junit-unit.xml

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results
          path: junit-unit.xml

  # Linting y type checking
  lint:
    name: Lint & Type Check
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install ruff mypy

      - name: Run Ruff
        run: |
          ruff check src/ tests/ api-server/ --output-format=github

      - name: Run MyPy
        run: |
          # Informational: many false positives from dynamic imports and missing stubs.
          # Not blocking until baseline is established.
          mypy src/narrative_assistant --ignore-missing-imports || true

  # Tests de integración (requieren modelos NLP)
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    # Solo en push a master, no en PRs (son lentos)
    if: github.event_name == 'push' && github.ref == 'refs/heads/master'

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -e ".[dev]"

      - name: Download spaCy model
        run: |
          python -m spacy download es_core_news_sm

      - name: Run integration tests
        run: |
          # Informational: integration tests require NLP models (~2GB) not available in CI.
          # Full integration suite runs locally. CI captures what it can.
          pytest tests/integration -v --tb=short -x --junitxml=junit-integration.xml || true

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: junit-integration.xml

  # Tests de performance (solo en master, manual o scheduled)
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 60
    # Solo manual o en releases
    if: github.event_name == 'workflow_dispatch'

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -e ".[dev]"
          python -m spacy download es_core_news_sm

      - name: Run performance tests
        run: |
          # Informational: performance tests are slow (~60min) and track regressions.
          # Only run on manual trigger (workflow_dispatch). Results uploaded as artifacts.
          pytest tests/performance -v --tb=short -m slow --junitxml=junit-perf.xml || true

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: junit-perf.xml

  # Resumen final
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, lint]
    if: always()

    steps:
      - name: Check results
        run: |
          if [ "${{ needs.unit-tests.result }}" == "failure" ]; then
            echo "Unit tests failed"
            exit 1
          fi
          if [ "${{ needs.lint.result }}" == "failure" ]; then
            echo "Lint failed"
            exit 1
          fi
          echo "All checks passed!"
