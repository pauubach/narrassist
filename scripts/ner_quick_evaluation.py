#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Evaluaci√≥n r√°pida de NER en archivos DOCX (sin validaci√≥n LLM).

Para obtener resultados m√°s r√°pido, deshabilitamos la validaci√≥n LLM.
"""

import sys
import io
import logging
from collections import defaultdict
from pathlib import Path

# Forzar salida UTF-8
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Configurar logging
logging.basicConfig(
    level=logging.WARNING,  # Solo warnings y errors para reducir ruido
    format='%(levelname)s - %(name)s - %(message)s'
)
logger = logging.getLogger(__name__)

# A√±adir src al path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from narrative_assistant.parsers.docx_parser import DocxParser
from narrative_assistant.nlp.ner import NERExtractor, EntityLabel


def format_number(num: int) -> str:
    return f"{num:,}".replace(",", ".")


def print_header(text: str, char: str = "="):
    line = char * 80
    print(f"\n{line}")
    print(f"{text.center(80)}")
    print(f"{line}\n")


def print_section(text: str):
    print(f"\n{'‚îÄ' * 80}")
    print(f"‚ñ∂ {text}")
    print(f"{'‚îÄ' * 80}")


def main():
    print_header("EVALUACI√ìN R√ÅPIDA DE NER (Sin validaci√≥n LLM)", "‚ïê")

    # Inicializar extractor
    print("üîß Inicializando NER Extractor (sin validaci√≥n LLM)...")
    extractor = NERExtractor()
    print("  ‚úì Listo\n")

    # Buscar archivo principal
    test_books_dir = Path(__file__).parent.parent / "test_books"
    doc_path = test_books_dir / "la_regenta_sample.docx"

    if not doc_path.exists():
        print(f"‚ùå ERROR: No se encontr√≥ {doc_path}")
        return 1

    print(f"üìÑ Analizando: {doc_path.name}\n")

    # Parse documento
    print("  Parseando documento...")
    parser = DocxParser()
    parse_result = parser.parse(doc_path)

    if parse_result.is_failure:
        print(f"  ‚ùå Error: {parse_result.error}")
        return 1

    raw_doc = parse_result.value
    full_text = raw_doc.full_text

    print(f"  ‚úì {format_number(len(full_text))} caracteres")
    print(f"  ‚úì {format_number(len(raw_doc.paragraphs))} p√°rrafos")

    # Extraer entidades SIN validaci√≥n
    print(f"\n  Extrayendo entidades (sin validaci√≥n LLM)...")
    ner_result = extractor.extract_entities(full_text, enable_validation=False)

    if ner_result.is_failure:
        print(f"  ‚ùå Error: {ner_result.error}")
        return 1

    result_value = ner_result.value
    entities = result_value.entities

    print(f"  ‚úì {len(entities)} entidades extra√≠das\n")

    # An√°lisis por tipo
    print_header("DISTRIBUCI√ìN POR TIPO")

    by_type = defaultdict(list)
    for entity in entities:
        by_type[entity.label].append(entity)

    for label in EntityLabel:
        count = len(by_type.get(label, []))
        pct = (count / len(entities) * 100) if entities else 0.0
        bar = "‚ñà" * int(pct / 2)
        print(f"  {label.value:<6} {count:>5} ({pct:>5.1f}%)  {bar}")

    # An√°lisis por fuente
    print_header("DISTRIBUCI√ìN POR FUENTE")

    by_source = defaultdict(int)
    for entity in entities:
        by_source[entity.source] += 1

    for source, count in sorted(by_source.items(), key=lambda x: x[1], reverse=True):
        pct = (count / len(entities) * 100) if entities else 0.0
        bar = "‚ñà" * int(pct / 2)
        print(f"  {source:<20} {count:>5} ({pct:>5.1f}%)  {bar}")

    # Muestras por tipo
    print_header("MUESTRAS DE ENTIDADES")

    for label in EntityLabel:
        label_name = {
            EntityLabel.PER: "PERSONAS",
            EntityLabel.LOC: "LUGARES",
            EntityLabel.ORG: "ORGANIZACIONES",
            EntityLabel.MISC: "MISCEL√ÅNEA"
        }.get(label, label.value)

        filtered = [e for e in entities if e.label == label]
        if not filtered:
            continue

        print(f"\n{label.value} - {label_name} (Total: {len(filtered)})")

        # Mostrar top 15 por confianza
        filtered.sort(key=lambda x: x.confidence, reverse=True)
        unique_texts = []
        seen = set()
        for e in filtered:
            text_lower = e.text.lower()
            if text_lower not in seen:
                seen.add(text_lower)
                unique_texts.append((e.text, e.confidence, e.source))
                if len(unique_texts) >= 15:
                    break

        for text, conf, source in unique_texts:
            conf_bar = "‚ñà" * int(conf * 10)
            print(f"  ‚Ä¢ {text:<35} [{conf_bar:<10}] {conf:.2f} ({source})")

    # Confidence scores
    print_header("AN√ÅLISIS DE CONFIANZA")

    confidences = [e.confidence for e in entities]
    avg_conf = sum(confidences) / len(confidences) if confidences else 0
    min_conf = min(confidences) if confidences else 0
    max_conf = max(confidences) if confidences else 0

    print(f"  Promedio:  {avg_conf:.3f}")
    print(f"  M√≠nima:    {min_conf:.3f}")
    print(f"  M√°xima:    {max_conf:.3f}")

    # Entidades de baja confianza
    low_conf = [e for e in entities if e.confidence < 0.7]
    if low_conf:
        print(f"\n  Entidades de baja confianza (<0.7): {len(low_conf)}")
        print(f"  Muestras:")
        for e in low_conf[:10]:
            print(f"    ‚Ä¢ {e.text:<30} {e.confidence:.2f} ({e.label.value}, {e.source})")

    # Estad√≠sticas finales
    print_header("RESUMEN")

    density = (len(entities) / len(full_text) * 1000) if len(full_text) > 0 else 0
    unique_count = len(result_value.unique_entities)

    print(f"  Total entidades:     {len(entities)}")
    print(f"  Entidades √∫nicas:    {unique_count}")
    print(f"  Densidad:            {density:.2f} entidades/1000 chars")
    print(f"  Caracteres:          {format_number(len(full_text))}")

    print_header("EVALUACI√ìN COMPLETADA", "‚ïê")
    print()

    return 0


if __name__ == "__main__":
    sys.exit(main())
